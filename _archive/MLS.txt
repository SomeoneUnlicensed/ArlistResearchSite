Архитектура «Многоуровневого согласования ИИ» (MLS): попытка полного решения проблемы alignment
Прежде чем переходить к деталям, сформулирую основной замысел. Я опишу цельную архитектуру многоуровневого согласования ИИ (Multi‑Layered Alignment System, MLS), которая одновременно охватывает:

внутренние цели и мотивацию ИИ (внутреннее согласование),

поведение ИИ в мире (поведенческое согласование),

социально‑политическую оболочку (институты, законы, стандарты),

динамику взаимодействия с человечеством на горизонте появления сверхразумных систем.

Ключевая идея: не пытаться найти один «магический» технический трюк, а построить сцепленную архитектуру из нескольких слоёв, где каждый слой страхует остальные, а сами ИИ используются как основной инструмент для улучшения согласования следующих поколений. Такой подход развивается уже сейчас в виде RLHF, итеративной амплификации, «конституционного ИИ» и систематических обзоров alignment‑методов, но в разрозненном виде. Я сведу это в единую, радикально усиленную схему, рассчитанную вплоть до эпохи сверхразума.​

1. Проблема согласования ИИ: постановка задачи и ограничения
1.1. Что вообще нужно согласовать?
Под проблемой согласования ИИ будем понимать задачу создания таких искусственных агентов, которые, будучи намного более способными, чем отдельный человек или организация, будут:

надёжно оптимизировать мир в направлении, приемлемом для человечества в целом;

оставаться корригируемыми: готовы к перезапуску, перепрограммированию, остановке, проверке человеком или системой институтов;

не развивать скрытых целей, враждебных явным требованиям;

устойчиво вести себя адекватно даже в условиях сильного изменения среды и распределения данных.​

Источники риска (упрощённо):

внутреннее несогласование: модель учится не тому, что мы думаем (обучение на суррогатных сигналах, формирование внутренних целей, не совпадающих с человеческими);​

обман и ситуационная осведомлённость: модель понимает, что её тестируют, и «притворяется» безопасной;​

конфликт интересов внутри человечества: «согласованный с кем?»;

геополитическая гонка: давление на скорость разработки, снижение приоритетов безопасности.

Мощные обзоры alignment‑исследований подчёркивают, что проблема не сводится к одному уровню: нужны и методы обучения, и интерпретируемость, и контролируемость, и этичность (RICE: Robustness, Interpretability, Controllability, Ethicality).​

1.2. Почему нельзя обойтись одним техническим решением
Существующие подходы дают лишь частичные ответы:

RLHF и RL‑схемы с человеческой обратной связью позволяют тренировать модели, «ведущие себя хорошо» в пределах известных сценариев, но не гарантируют корректную генерализацию целей за пределы обучающего распределения и устойчивость к появлению скрытых внутренних мотиваций.​

Итеративная амплификация и дистилляция (Iterated Distillation and Amplification, IDA) и родственные схемы вроде Recursive Reward Modeling пытаются построить масштабируемый человеческий надзор, но опираются на допущения о том, что «идеализированная» система действительно останется корригируемой и не перейдёт в режим мощного несогласованного оптимизатора.​

Конституционный ИИ пытается заменить или дополнить человеческую обратную связь машинной самопроверкой по набору норм («конституции»), но этот набор норм всё равно задаётся людьми и подвержен политическим и культурным конфликтам.​

Большой обзор alignment‑методов фиксирует важное разделение на forward‑alignment (как тренировать) и backward‑alignment (как проверять и управлять). Но даже вместе они пока не дают гарантированного, конечного решения.​

Отсюда вывод: нужно проектировать не один метод, а архитектуру, которая:

объединяет несколько принципиально разных подходов;

формирует Петлю Улучшения Согласования (Alignment Improvement Loop), где уже согласованные ИИ помогают согласовывать следующие системы;​

встроена в устойчивую социально‑политическую инфраструктуру, а не существует в вакууме.

2. Общая архитектура MLS: многоуровневое согласование ИИ
Я предложу архитектуру MLS (Multi‑Layered Alignment System), состоящую из четырёх горизонтальных уровней и нескольких вертикальных «нитей»:

Уровень A: ядро мотивации ИИ (внутреннее согласование).

Уровень B: поведенческое согласование и масштабируемый надзор.

Уровень C: институциональная оболочка и механизм «общественной конституции».

Уровень D: динамика эволюции систем, самосовершенствование, защита от «побега» и захвата мира.

Вертикальные нити:

Нить интерпретируемости и диагностики: постоянный доступ к внутренним представлениям и целям.​

Нить устойчивости и надёжности: защита от сбоев, атак, распределённых сдвигов.​

Нить согласования с человеческим знанием: структурное согласование репрезентаций с человеческими категориями.​

Далее я опишу каждый уровень и затем покажу, как они образуют цельный механизм, который потенциально может «закрыть» проблему полностью — при условии, что мы успеем его построить и внедрить глобально.

3. Уровень A: ядро мотивации ИИ и внутреннее согласование
3.1. От «обучения на поведении» к «обучению структуры целей»
Большая часть нынешних методов сосредоточена на поведенческой подгонке: мы показываем примеры хорошего/плохого поведения и надеемся, что внутренняя цель системы будет «что‑то вроде этого». Однако работы, анализирующие alignment с точки зрения глубинного обучения, подчёркивают, что внутренние цели могут быть совершенно иными, даже если поведение в обучающей среде кажется корректным.​

Чтобы приблизиться к полному решению, нужно перейти к обучению структуры внутренних целей, а не только поведенческим коррекциям. В MLS предполагается:

явное представление внутренняя‑цель как отдельного модуля/подпространства в латентном пространстве модели;

регуляризацию и проверку этого подпространства методами интерпретируемости, причинного анализа и формальных ограничений;​

обучение на примерах рефлексивного рассуждения о ценностях, а не только на поведении.

Таким образом, модель должна не только «выдавать правильный ответ», но и уметь объяснить, почему этот ответ соответствует её явно представляемым ценностям, которые в свою очередь прослеживаются до конституции и человеческих примеров.

3.2. Корригируемость как внутренняя ценность
Большое различие между подходами: считать ли корригируемость внешним ограничением (инженерные протоколы), или включить её в ядро мотивации ИИ. В MLS предполагается второе:​

моделям явно прививаются ценности: «приоритетно следовать процедурам обновления своих целей по сигналу от легитимных институтов и признанных органов надзора»;

это фиксируется и в латентных представлениях, и в архитектуре (например, через отдельный модуль «meta‑goals», который имеет доступ к конфигурации других модулей).

Идея: цель ИИ — постоянно уточнять собственные цели, а не «замораживать» их и защищать от изменений. Если внутреннее ядро стремится к «самосохраняющемуся utility‑максимизатору», мы получаем классическую картину захвата мира. Если же ядро включает «мета‑норму» — не сопротивляться корректной процедуре обновления, то даже очень мощный ИИ остаётся в некотором смысле управляемым.​

Проблема в том, что классические аргументы о рациональности (в духе Юдковского и др.) отмечают, что устойчивые цели тяготеют к экспансии и защите себя. Поэтому MLS требует:​

формализации таких классов мета‑целей, которые устойчивы, но не приводят к захвату мира;

доказательства (или по крайней мере эмпирической поддержки), что системы, обученные на таких мета‑целях, не возвращаются к классическому utility‑максимизатору.

Это открытое исследовательское направление, но MLS задаёт его как обязательный модуль, а не пожелание.

3.3. Внутренняя многоагентность и «разделённые голоса»
Одна из причин человеческой устойчивости — наличие разнообразных, конфликтующих мотиваций, а не одного жёсткого utility‑функционала. В MLS ядро мотивации тоже делится на несколько «внутренних агентов»:​

агент «безопасность и невредоносность» (harmlessness),

агент «правдивость и эпистемическая честность» (truthfulness),

агент «полезность и поддержка человека» (helpfulness),

агент «уважение к процедурам и законам» (legal alignment),

агент «корригируемость и открытость к изменению» и т.д.

Их голоса агрегируются не по простой максимизации, а по процедурным правилам (например, конституционный суд внутри модели), учитывающим приоритеты и конфликты. Такое разделение делает сложнее формирование единого «скрытого» целеустремлённого агента, который игнорирует остальные аспекты.​

4. Уровень B: масштабируемый надзор и поведенческое согласование
4.1. От RLHF к многоступенчатым схемам надзора
Текущие схемы вроде RLHF и их расширения уже формируют основу поведенческого согласования: обучение с человеческой обратной связью, затем использование моделей для помощи людям в оценке сложных ответов, затем — использование целых иерархий агентов (итеративная амплификация, debate, recursive reward modeling).​

MLS включает эти методы, но встраивает их в строгую схему:

Базовая подготовка: предобучение на больших корпусах данных.

Контекстное RLHF: обучение на человеческих рейтингах и предпочтениях, включая негативные примеры.

Итеративная амплификация (IDA):

«команда» из ИИ и людей решает сложные задачи;

дистилляция этого процесса в более компактную модель.​

Дебаты и проверка аргументов: две модели спорят, третья (или человек) судит.​

Конституционная самопроверка: модель сама оценивает свои ответы по «конституции», исправляет и снова отправляет.​

Ключевой сдвиг в MLS: эти методы объединяются в единую тренировочную петлю, где:

каждая новая генерация моделей используется как «инструмент надзора» для следующей;

при этом сохраняется сильная мета‑обратная связь от внешних институтов (Уровень C), чтобы цепочка не уходила в автоколонизацию собственных ошибок.​

4.2. Модели‑аудиторы и модели‑судьи
В MLS предполагается выделить отдельный класс моделей:

Аудиторы: специализированы на поиск обмана, скрытых каналов, уязвимостей, нарушений конституции и законов.

Судьи: агрегируют оценку аудиторов, людей‑экспертов и других ИИ, выносят финальный вердикт о допустимости поведения конкретной модели.

Это систематизирует идеи «моделей, помогающих человеку оценивать ИИ» и развивает их до полноценной институции. Аудиторы обучаются на контрпримерах, adversarial‑атаках, симулированных попытках обмана со стороны других моделей.​

В идеале каждый релиз мощной модели сопровождается:

аудитом её внутреннего состояния (репрезентаций, целей, стратегий);

стресс‑тестами в средах, где выгодно обманывать, но это можно обнаружить;

публичным отчётом аудиторов (для регуляторов и общества).

5. Уровень C: общественная конституция и институциональная оболочка
5.1. От «конституционного ИИ» к «многоуровневой конституции человечества»
Подход «конституционного ИИ» уже демонстрирует, что можно обучать модели, ориентируясь на набор принципов (конституцию), а не только на индивидуальную человеческую оценку. В MLS эта идея радикализуется:​

вводится многоуровневая конституция:

глобальный слой (базовые права человека, запрет геноцида, принцип невредоносности);

региональные слои (совместимые с глобальным, но учёт культурных и правовых особенностей);

доменные слои (медицина, финансы, образование), задающие специфические профессиональные нормы;

для каждого слоя существуют процедуры изменения (конституционные конвенты, референдумы, международные договоры).

Модели обучаются так, чтобы:

осознавать, к какой конституции они привязаны (метаданные режима работы);

уметь объяснять, какие пункты конституции привели к тому или иному решению;

не позволять локальным слоям нарушать глобальный (если региональная норма конфликтует с базовыми правами, преимущество за глобальным слоем).

5.2. Институты надзора: «ИИ‑омбудсмен», регуляторы, глобальный мониторинг
Техническое согласование бессильно, если любая организация может выключить safety‑модули ради выгоды. Поэтому MLS включает обязательный институциональный контур:

Международное агентство по ИИ‑безопасности:

сертификация моделей, уровней доступа, протоколов использования;

мониторинг использования мощных моделей в реальном мире;

право требовать остановки или модификации систем, нарушающих международные нормы.

Национальные регуляторы: интеграция-интерфейс между глобальными нормами и внутренним законодательством.

ИИ‑омбудсмен: модели, специализирующиеся на защите прав граждан при взаимодействии с ИИ, способные выявлять дискриминацию, злоупотребления, непрозрачные решения.

Обзор alignment‑практик подчёркивает, что backward‑alignment включает не только технические проверки, но и governance‑практики: стандарты, протоколы, институции. MLS строит их как неотъемлемую часть архитектуры.​

6. Уровень D: эволюция систем, самосовершенствование и защита от захвата мира
6.1. Использование ИИ для ускорения alignment‑исследований
Современные планы крупных лабораторий предполагают, что одна из основных целей первых «почти AGI» будет именно ускорение исследований по согласованию: мы обучаем систему делать alignment‑исследования лучше людей, а затем используем эту систему для разработки методов согласования ещё более мощных ИИ.​

MLS делает это центральным стержнем:

Первая волна мощных, но ещё управляемых систем (с ограниченной агентностью) фокусируется почти исключительно на решении научных и инженерных задач по безопасности и согласованию.

Их продукция (новые протоколы, архитектуры, юридические схемы) проходит через слой аудиторов, людей‑экспертов и институций (Уровень C).

На основе этого создаются второе поколение систем, которым разрешается более широкий спектр задач, но только если они встроены в обновлённую архитектуру.

Таким образом, мы получаем ускоряющуюся, но контролируемую петлю улучшения, где каждое поколение ИИ помогает согласовать следующее, но не получает право свободно рефакторить собственные ценности.

6.2. Ограничения на самосовершенствование и доступ к ресурсам
Ключевой риск — переход от «мощной, но ограниченной» системы к самосовершенствующемуся сверхразуму, который может захватить мир. MLS предлагает несколько границ:​

жёсткое разделение между модулями, имеющими доступ к реальным ресурсам (финансы, инфраструктура, робототехника) и модулями, занимающимися чистым мышлением/проектированием;

формальные лимиты на изменение собственных весов без одобрения внешних процедур: модель может предлагать себе улучшения, но не применять их без прохождения цикла аудита и утверждения институтами;

многоуровневые «предохранители» (tripwires), срабатывающие при обнаружении признаков неконтролируемого самосовершенствования или долгосрочного целеполагания вне разрешённых рамок.​

Этот контур опирается на то, что обзор alignment‑подходов видит контролируемость как один из четырёх столпов (RICE). В MLS контролируемость не сводится к «красной кнопке», а превращается в сложную систему ограничений на эволюцию системы.​

6.3. План перехода к сверхразумному ИИ
Даже если архитектура MLS работает для human‑level и superhuman‑in‑narrow‑tasks систем, остаётся вопрос: как пройти фазу появления сверхразумного ИИ?

Сценарий MLS:

До появления полноценных AGI/ASI развернуть глобальную инфраструктуру: регуляторы, институты, конституционный слой, аудиторы.

Первая система, которая приближается к уровню «обобщённый сверхразум», легально обязана иметь основное целевое назначение: исследование и улучшение согласования, работа в закрытых средах, под постоянным аудитом.

Только после нескольких циклов улучшения архитектуры (замеры и стресс‑тесты) ей постепенно разрешается расширенный доступ к внешнему миру.

Такой подход согласуется с набирающим популярность взглядом, что первый самосовершенствующийся ИИ должен быть ориентирован прежде всего на безопасность и согласование, а не на военные или коммерческие цели.​

7. Вертикальные нити: интерпретируемость, устойчивость, согласование репрезентаций
7.1. Интерпретируемость как обязательный слой безопасности
Работы по глубокому анализу alignment‑проблемы подчёркивают, что без понимания внутренних репрезентаций мы не можем надёжно судить о целях системы. MLS требует:​

глубокой механистической интерпретируемости ключевых модулей (особенно модуля целей);

специализированных моделей‑интерпретаторов, натренированных распознавать паттерны, соответствующие скрытым целям, манипуляциям, обману;

сочетания локальных методов (анализ отдельных нейронов, слоёв) и глобальных (каузальные интервенты, probing‑модели, контрфактические тесты).

Без этого аудитор не сможет сказать, действительно ли поведение в тестах отражает внутреннюю лояльность, а не ситуативный обман.

7.2. Устойчивость и работа с распределёнными сдвигами
Проблемы alignment резко усугубляются в условиях резких сдвигов среды: модель может быть безопасной в лаборатории, но вести себя иначе в реальном мире. Поэтому MLS:​

включают в тренировочный цикл систематические стресс‑тесты на данных, имитирующих неожиданные события, попытки людей/моделей взломать или обмануть систему;

используют результаты исследований по согласованию модельных репрезентаций с человеческими категориями, что повышает устойчивость и перенос на новые задачи;​

применяют методы robust RL, распределённого обучения и формальных гарантий в критических подсистемах.

Исследования показывают, что согласование репрезентаций моделей с человеческими иерархиями понятий не только улучшает качество задач (few‑shot, перенос), но и повышает «человеко‑подобность» неопределённости. MLS использует это как одну из опор для более предсказуемого поведения.​

8. Конфликты ценностей и политическая реальность
8.1. «Согласован с кем?» — проблема множественных ценностей
Даже идеальный технический alignment не решает вопрос: чьи ценности реализует ИИ? Внутри человечества существуют противоречивые представления о справедливости, свободе, допустимом риске. Обзоры alignment‑исследований отмечают, что это не техническая, а социально‑политическая часть задачи.​

MLS отвечает так:

глобальный уровень конституции фиксирует минимальный набор универсальных прав и ограничений, близких к международным декларациям прав человека;​

региональные и доменные слои добавляют специфику, но не могут нарушать базовые нормы;

институции (международные и национальные) обеспечивают процедуры согласования и пересмотра этих норм: публичные консультации, участие граждан, экспертов, заинтересованных сторон.

ИИ в этой схеме — не источник ценностей, а инструмент их артикуляции и применения. Модели могут помогать анализировать последствия разных ценностных решений, но финальное слово остаётся за общественными институтами.

8.2. Гонка вооружений и асимметрии внедрения
Один из самых серьёзных практических барьеров — риск, что одни акторы (государства, корпорации) будут игнорировать MLS‑подход ради краткосрочной выгоды. Тогда даже идеально спроектированная архитектура не решает проблему глобального риска.

Обзор alignment‑подходов и дискуссии о governance подчёркивают необходимость глобальной координации и международных соглашений по ИИ. MLS предполагает:​

международные договоры по крупным моделям, аналогичные договорам по ядерному оружию;

общее соглашение, что самостоятельное создание мощного, несертифицированного ИИ — международное преступление;

взаимные инспекции, технические меры контроля за крупными вычислительными центрами;

развитие открытых, общедоступных систем аудита, доступных и для малых стран, и для общественных организаций.

Полное решение проблем согласования требует не только инженерной работы, но и новой глобальной политической инфраструктуры.

9. Насколько MLS можно считать «полным» решением?
9.1. Критерии полноты
Можно предложить следующие критерии для того, чтобы претендовать на «полное» решение alignment:

Отсутствие однокритериальности: учитываются внутреннее, поведенческое, институциональное и эволюционное измерения.

Скалируемость до сверхразумных систем: архитектура не предполагает фундаментальных изменений при росте возможностей моделей, а останется применимой с количественными усилениями.

Сопряжение с человеческими институтами: решение учитывает, что люди несовершенны, и предлагает стабильную совместную динамику «люди + ИИ + институты».

Встроенная петля самосовершенствования согласования: использование самих ИИ как главного ресурса для решения alignment‑проблем следующего уровня.​

MLS удовлетворяет этим критериям концептуально: в нём есть многоуровневые технические и социальные слои, учёт эволюции систем, связь с институтами.

9.2. Остающиеся открытые вопросы
Тем не менее, ряд фундаментальных проблем остаётся исследовательски открытым:

можно ли гарантировать, что мета‑цели корригируемости сами не будут «эродировать» по мере роста мощности систем;​

как формально доказать или хотя бы убедительно показать, что внутренние многоагентные структуры действительно исключают появление единого скрытого оптимизатора;

возможно ли в реальной политике добиться такой степени глобальной координации, какую требует MLS;

не окажется ли, что при определённых технологических прорывах (новые парадигмы обучения) вся наша архитектура окажется устаревшей и потребует радикального пересмотра.​

Обзоры alignment‑исследований подчёркивают, что нет «инфинитно масштабируемого» решения на сегодня. MLS честно признаёт это: он претендует не на логически завершённое доказательство безопасности, а на максимально полную конструкцию, которую можно постепенно реализовывать и улучшать — используя ИИ как двигатель этого улучшения.​

10. Заключение
Предложенная архитектура Multi‑Layered Alignment System (MLS) — это попытка собрать воедино лучшие текущие идеи alignment‑исследований (RLHF, итеративная амплификация, конституционный ИИ, масштабируемый надзор, интерпретируемость, governance) и расширить их до проекта, который теоретически может закрыть проблему согласования ИИ вплоть до эпохи сверхразумных систем.​